# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The dataset contains customer information on marketing campaigns of a financial institution. We seek to predict whether the customer will contrat a product or not (variable `y`). Therefore, we are facing a classification task.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best perfoming model  was a VotingEnsemble composed of a pool of 7 weaker-learning pipelines that together form a strong learner. The accuracy obtained was 91.9391%.

The resulting model is comoposed of 7 individual models and a set of weights that allows to aggregate the result of each individual model into a unique classification result. The models that compose the ensemble are 5 `XGBoostClassifier`, a `LightGBMClassifier` and a `SGDClassifierWrapper`.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

An initial code `train.py` was provided, which is responsible for:
  * Creating a `TabularDatasetFactory` from an online CSV file.
  * Data cleansing and enconde categorical data into numeric format using one-hot encoding or ordinal encoding when applicable.
  * Splitting the resulting data into train and test subsets. This is a common practice to ensure that the trained model is evaluated with a different subset than the one used to train it.
  * Using the function `LogisticRegressionClassifier` from `scikit-learn`, which implements the Logistic Regressi√≥n algorithm. This model tries to find the underlying relationship between the target variable `y` and the rest of the features in the dataset.
  * Saving the model as a pickle file, recording the value of its hyperparameter.

The input arguments of `train.py` script are the two hyperparameters that will be used to train the `LogisticRegressionClassifier`.
  * The first is the hyperparameter `C`, which represents the inverse of regularization strength. Smaller values cause stronger regularization.
  * The second is the hyperparameter `max_iter`, which represents the maximum number of iterations to converge.

Finally, the implementation of the hyperdrive job is located in a Jupyter notebook called `udacity-project.ipynb`. This job will execute the `train.py` script several times with different hyperparameter configurations. These configurations are specified by a parameter sample (`RandomParameterSampling`), that will randomly choose from the range of values specified by the user. Moreover, a policy is defined (`BanditPolicy`) for early stopping in case some specific conditions are met.


**What are the benefits of the parameter sampler you chose?**

It explores randomly a hyperparameter space, saving a lot of time compared to an exhaustive search.

* The hyperparameter `C` take a uniform distribution bounded to two values.
* The `max_iter` parameter can be chosen from a given set that ranges from 10 to 100.

**What are the benefits of the early stopping policy you chose?**
We chose a `BanditPolicy`, that defines an early stopping policy based on slack criteria and a frequency and delay interval for evaluation.

The `slack_factor` is the ratio used to calculate the allowed distance from the best performing run. We chose a value of 10%, so if in each time the policy is evaluated (as defined by `evaluation_interval`), the metric falls below the slack respect the best performing model, the job is terminated. This allows us to finish before reaching the maximum number of iterations (`max_total_runs`)  if the model is not improving with each iteration, therefore saving computing time.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoML configuration that has been defined establishes that the maximum time the experiment should run is 30 minutes. Moreover, we banned the use of `LogisticRegresionClassifier` model, since it is the one that we have tested in the previous section.

So as to try to work under the same conditions as the previous experiment, we chose the `accuracy` as the primary metric to maximize, and we decided not to use cross-validation, as it would be easier to compare pure performance of both solutions if we kept all modelling decisions as similar as possible.

AutoML generated a total of 36 pipelines, given that they have been the models that it has managed to train within the defined 30-minute limit. The best perfoming one (Accuracy 91.9391%) was a VotingEnsemble composed of a pool of 7 weaker-learning pipelines that together form a strong learner. The second best performing model (Accuracy 91.77%) was an XGBoost Classifier with a StandardScalerWrapper preprocessing.

The hyperparameters of the VotingEnsemble can be obtained by printing `automl_run.get_output()`. This will show the individual hyperparameters of each of the 7 pipelines that compose the model. The models that compose the ensemble are 5 `XGBoostClassifier`, a `LightGBMClassifier` and a `SGDClassifierWrapper`. The weights of the VotingEnsemble are the following:
```weights=[0.1111111111111111, 0.2222222222222222, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]```. For more information, please check the corresponding section of `udacity-project.ipynb`.

The highest weights are assigned to a `XGBoostClassifier` and the `LightGBMClassifier` model.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

In terms of accuracy, the best performing model is the `VotingEnsemble` obtained from the AutoML process (accuracy 91.9391%). However, the one obtained with the SKlean pipeline (`LogisticRegression`) is much more simpler and the reduction in accuracy is not that much (Accuracy 91,089%). Depending on the needs of the project we will recommend one or another. If the data changes a lot, maybe we prefer to have a poorer perfomance but a model that takes into account all that new data instantaneously. However, if we prefer the model that gives us the best predictions, no matter the time spent on training the model, we will choose the `VotingEnsemble`.

THe main differences between the two approaches:

* Both of them have the same initial preprocessing part. However, the model obtained with AutoML includes also an additional step, that scales the features to the same range of values, ensuring that all features has the same initial importance no matter its original range of values.

* The AutoML best performing model is an ensemble of weaker learners, whereas the `LogisticRegressionClassifier` consists in just one model that may struggle to identify all the non-linearities in our data.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

* We could start by applying some scaling method to the `LogisticRegressionClassifier` pipeline and check if the accuracy of the model reaches similar values to those of AutoML. Normally, this type of model (Logistic Regression) does not require scaling of variables, but this technique can favor the convergence of the underlying optimization methods that it uses and, therefore, suppose an improvement in accuracy or faster results.

* The dataset is quite imbalanced. We can check this fact in the notebook `udacity-project.ipynb`. The AutoML job shows as that here are 2473 positive samples out of 22076, which is only the 11.2% of the total set. In order to better evaluate the models I would suggest to use another performance metric such as `f1-score` or `AUC`, since these metrics take into account the performance of the model in each class separately. We could also try to downsample the majority class or oversample the minority class, so as to achieve a balanced dataset.

* As mentioned before, if we have to choose one of these models for production purposes, I would definitely not select the VotingEnsemble model, since the slight improvement in the accuracy of the predictions does not justify losing inference speed and model explainability. It is better to choose a model that gives slightly worse predictions but it is easier to explain, such as, XGBoost or LightGBM.